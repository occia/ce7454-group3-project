{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE7454 2019 Project -- Group 3\n",
    "\n",
    "**Add the full name here**\n",
    "\n",
    "Please find all the models and data at [https://github.com/occia/ce7454-group3-project](https://github.com/occia/ce7454-group3-project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/b5/bab3477466a4d9e705d40829ac65683155e7977acbc07f05b06fabded1be/pandas-0.25.3-cp37-cp37m-macosx_10_9_x86_64.whl (10.2MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.2MB 6.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 512kB 9.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-0.25.3 pytz-2019.3\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import urllib.request as urllib\n",
    "import scrapy\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import argparse\n",
    "!pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "from random import randint\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Description\n",
    "\n",
    "Here briefly introduces the background, and throw out the 2 project questions:\n",
    "- How accurate the modern neural network models could be? (How to relate this with the Identity Acquisition?)\n",
    "- What's the performance of age authentication (below/above 18) for current neural networks?\n",
    "\n",
    "TODO DAVID: Story at the beginning, leading through story during evaluation and conclusion section + Future directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Acquisition\n",
    "\n",
    "\n",
    "For the project, we prepare 2 kinds of data:\n",
    "- training data\n",
    "- validation/testing data\n",
    "\n",
    "#### Bechmark Data\n",
    "\n",
    "Due to the requirement of the large amount of labeled data, we merged 3 existing labeled benchmark datasets as our training data, including [All-Age-Faces](https://github.com/JingchunCheng/All-Age-Faces-Dataset), [FGNET](https://yanweifu.github.io/FG_NET_data/index.html), [UTK Face](https://susanqq.github.io/UTKFace).\n",
    "\n",
    "In total, the amount of labelled images from these 3 benchmarks is 38000, 32818 is used for training and 5182 is used for validation/testing.\n",
    "\n",
    "<span style=\"color:red\"> P: Should we mention that we selected the benchmark datasets for age classification? We can also insert the table from the papers about the bechmarks with their characteristics to justify our choice.</span>\n",
    "\n",
    "#### Real-world Data\n",
    "\n",
    "To validate the performance on the real-world data, we selected Instagram as a source for an additional dataset of unfiltered face images.\n",
    "We assumed that we may retrieve the age from bio and the actual image from the picture profile.\n",
    "\n",
    "##### Usernames Scraping \n",
    "\n",
    "As we intended to select the users from a particular country, we chose one influencer per country.\n",
    "We considered the following countries:\n",
    "\n",
    "European:\n",
    "* Australia - [@eddiebthe3rd](https://www.instagram.com/eddiebthe3rd/)\n",
    "* Canada - [@od_officiel](https://www.instagram.com/od_officiel/)\n",
    "* Germany - [@kontrak](https://www.instagram.com/kontrak/)\n",
    "* Russia - [@\\_agentgirl\\_](https://www.instagram.com/_agentgirl_/)\n",
    "\n",
    "Asian:\n",
    "* China - [@bingbing_fan](https://www.instagram.com/bingbing_fan/)\n",
    "* Indonesia - [@jokowi](http://instagram.com/jokowi)\n",
    "* India - [@narendramodi](http://instagram.com/narendramodi)\n",
    "\n",
    "Middle Eastern:\n",
    "* Iran - [@golfarahani](http://instagram.com/golfarahani)\n",
    "\n",
    "African:\n",
    "* Ethiopia - [@addisalem_getaneh](https://www.instagram.com/addisalem_getaneh/)\n",
    "* Nigeria - [@iambisola](https://www.instagram.com/iambisola)\n",
    "\n",
    "Hispanic:\n",
    "* Brazil - [@carlinhosmaiaof](https://www.instagram.com/carlinhosmaiaof/)\n",
    "\n",
    "To identify the necessary influencers we used websites such as [HypeAuditor](https://hypeauditor.com/top-instagram-all-australia/) and [Heepsy](https://www.heepsy.com/ranking/top-instagram-influencers-in-ethiopia) -- we have used it to get the information about the audience, as we were interested in the influencers with the audience which is at least 80% local.\n",
    "\n",
    "Instagram does not allow scrapping and detects spiders, so we used a third-party application for Instagram called [Imgtagram](https://imgtagram.com/followers/justinbieber). It also allows us to retrieve the usernames in a most efficient way. We also attempted to retrieve usernames by means of visual testing, i.e. Selenium, that imitates user behavior, but it was much slower. \n",
    "\n",
    "To do so, we opened a web page of a particular user's followers and scrolled down the page till the number of users shown reaches 125k. We ran <span style=\"color:red\"> the following JS script </span> in a developer's console of a browser:\n",
    "\n",
    "<pre>\n",
    "function scrapLinksAndScroll() {\n",
    "  window.scrollTo(0, document.body.scrollHeight);\n",
    "}\n",
    "\n",
    "setInterval(scrapLinksAndScroll, 3); </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Biography and Name Scraping\n",
    "After we collected the usernames, we applied a library called scrapy that allows to scrap the webpage content based on html elements.\n",
    "    scrapy allows us to do so in a multiprocessing way. The source code of a scraper looks as follows and requires a command \n",
    " \n",
    " TODO WRAP IT\n",
    "    <span style=\"color:red\"> scrapy crawl -o country.json </span>\n",
    "\n",
    "In this way, we write the collected data of users per country in a json file storing the information regarding their username, name, bio, country, and image URL.\n",
    "\n",
    "Link src DIR ./scrapy dir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We processed all the countries one by one as the data required careful validation.\n",
    "We showcase the data scraping process on a small (30 users) subset of Canadian instagram users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'canada'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../data/test/usernames/canada.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7d9e2c3be034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# cd scrapy/instascraper/instascraper/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mQuotesSpider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcountry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'canada'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"profiles\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-7d9e2c3be034>\u001b[0m in \u001b[0;36mQuotesSpider\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"profiles\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../../data/test/usernames/%s.txt'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mstart_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/test/usernames/canada.txt'"
     ]
    }
   ],
   "source": [
    "# cd scrapy/instascraper/instascraper/\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    country = 'canada'\n",
    "    name = \"profiles\"\n",
    "    file_path = '../../../data/test/usernames/%s.txt' %country\n",
    "    with open(file_path) as f:\n",
    "        start_urls = []\n",
    "        for u in f.readlines():\n",
    "            start_urls.append('https://imgtagram.com/u/' + u)\n",
    "\n",
    "    def parse(self, response):\n",
    "        country = 'canada'\n",
    "        for quote in response.css('div.text-block'):\n",
    "            yield {\n",
    "                'username': quote.css('h3::text').get(),\n",
    "                'name': quote.css('h1::text').get(),\n",
    "                'bio': quote.css('p.descp::text').get(),\n",
    "                'image': response.css('img.icon::attr(src)').get(),\n",
    "                'country': country\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting information is stored in a JSON file that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>bio</th>\n",
       "      <th>image</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@x_.bellita._x</td>\n",
       "      <td>bella‚ù§Ô∏è</td>\n",
       "      <td>None</td>\n",
       "      <td>https://scontent-cdg2-1.cdninstagram.com/vp/02...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@_juliette_girard</td>\n",
       "      <td>Juliette Girard</td>\n",
       "      <td>None</td>\n",
       "      <td>https://scontent-sin2-1.cdninstagram.com/vp/fa...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@justine_marcoux10</td>\n",
       "      <td>Justine Marcoux</td>\n",
       "      <td>enjoy the little thingsüåû\\n_13 y/o\\n_üéø\\n_</td>\n",
       "      <td>https://scontent-cdg2-1.cdninstagram.com/vp/fa...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@poutine_myra</td>\n",
       "      <td>Jeramiü•∞</td>\n",
       "      <td></td>\n",
       "      <td>https://scontent-cdg2-1.cdninstagram.com/vp/6f...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@coraliebillette</td>\n",
       "      <td>Coralie :)</td>\n",
       "      <td>Dancerüíõ\\n</td>\n",
       "      <td>https://scontent-cdg2-1.cdninstagram.com/vp/8b...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>@enyalachance._</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://scontent-cdg2-1.cdninstagram.com/vp/07...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>@rraaphb</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>https://scontent-cdg2-1.cdninstagram.com/vp/eb...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>@audreyann.paquet</td>\n",
       "      <td>Audrey-Ann Paquet</td>\n",
       "      <td>27 ans . Rimouski üåº                           ...</td>\n",
       "      <td>https://scontent-cdg2-1.cdninstagram.com/vp/9d...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>@marie_pierjolin</td>\n",
       "      <td>Marie-Pier Jolin</td>\n",
       "      <td>Une Pinkie heureuse üåª</td>\n",
       "      <td>https://scontent-cdg2-1.cdninstagram.com/vp/15...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>@lolobouu</td>\n",
       "      <td>üåπ LAURENCE. B</td>\n",
       "      <td>1997 | Nursing student | Gryffindor\\nüåø Saguena...</td>\n",
       "      <td>https://scontent-cdg2-1.cdninstagram.com/vp/e1...</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               username                name  \\\n",
       "0        @x_.bellita._x            bella‚ù§Ô∏è    \n",
       "1     @_juliette_girard    Juliette Girard    \n",
       "2    @justine_marcoux10    Justine Marcoux    \n",
       "3         @poutine_myra            Jeramiü•∞    \n",
       "4      @coraliebillette         Coralie :)    \n",
       "..                  ...                 ...   \n",
       "96      @enyalachance._                       \n",
       "97             @rraaphb                       \n",
       "98    @audreyann.paquet  Audrey-Ann Paquet    \n",
       "99     @marie_pierjolin   Marie-Pier Jolin    \n",
       "100           @lolobouu      üåπ LAURENCE. B    \n",
       "\n",
       "                                                   bio  \\\n",
       "0                                                 None   \n",
       "1                                                 None   \n",
       "2           enjoy the little thingsüåû\\n_13 y/o\\n_üéø\\n_     \n",
       "3                                                        \n",
       "4                                           Dancerüíõ\\n    \n",
       "..                                                 ...   \n",
       "96                                                       \n",
       "97                                                None   \n",
       "98   27 ans . Rimouski üåº                           ...   \n",
       "99                               Une Pinkie heureuse üåª   \n",
       "100  1997 | Nursing student | Gryffindor\\nüåø Saguena...   \n",
       "\n",
       "                                                 image country  \n",
       "0    https://scontent-cdg2-1.cdninstagram.com/vp/02...  canada  \n",
       "1    https://scontent-sin2-1.cdninstagram.com/vp/fa...  canada  \n",
       "2    https://scontent-cdg2-1.cdninstagram.com/vp/fa...  canada  \n",
       "3    https://scontent-cdg2-1.cdninstagram.com/vp/6f...  canada  \n",
       "4    https://scontent-cdg2-1.cdninstagram.com/vp/8b...  canada  \n",
       "..                                                 ...     ...  \n",
       "96   https://scontent-cdg2-1.cdninstagram.com/vp/07...  canada  \n",
       "97   https://scontent-cdg2-1.cdninstagram.com/vp/eb...  canada  \n",
       "98   https://scontent-cdg2-1.cdninstagram.com/vp/9d...  canada  \n",
       "99   https://scontent-cdg2-1.cdninstagram.com/vp/15...  canada  \n",
       "100  https://scontent-cdg2-1.cdninstagram.com/vp/e1...  canada  \n",
       "\n",
       "[101 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles_path = './data/test/bio/%s.json' %country\n",
    "\n",
    "with open(profiles_path, 'r') as file:\n",
    "    user_profiles = json.load(file)\n",
    "    retrieved_profiles = pd.DataFrame.from_dict(user_profiles)\n",
    "\n",
    "retrieved_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering the bio\n",
    "\n",
    "To identify names and bios that contain age, we have used a regular expression that looks for the numbers in the aforementioned fields that meet the following requirements:\n",
    "\n",
    "* The previous symbol is not an alphanumeric character or an underscore, except for the case when the previous two symbols are represent a control sequence (\\n, \\t or \\r)\n",
    "* The number is either in range 1930-1999, or 2000-2019, or 10-99\n",
    "* The number is not followed by a digit\n",
    "\n",
    "\n",
    "When collecting our dataset, we also did manual checking to confirm the results.\n",
    "\n",
    "TODO: Check if folder exists, and create if not exists for json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/test/bio/filtered/canada.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-33b640292bb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mfiltered_profiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_profiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/test/bio/filtered/canada.json'"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "filtered_profiles = []\n",
    "\n",
    "input_path = './data/test/bio/%s.json' %country\n",
    "output_path = './data/test/bio/filtered/%s.json' %country\n",
    "\n",
    "with open(input_path, 'r') as file:\n",
    "    user_profiles = json.load(file)\n",
    "    for user in user_profiles:\n",
    "        if user['bio'] and user['bio'] != ' ':\n",
    "            year_pattern = re.compile(\"(?:(?<!\\w)|(?<=\\\\[ntr]))(19[3-9]\\d|20[01]\\d)(?!\\d)\")\n",
    "            age_pattern = re.compile(\"(?:(?<!\\w)|(?<=\\\\[ntr]))([1-9]\\d)(?!\\d)\")\n",
    "            birth_year_bio = year_pattern.match(user['bio'])\n",
    "            str_name = str(user['name'])\n",
    "            birth_year_name = year_pattern.match(str_name)\n",
    "            age_bio = age_pattern.match(user['bio'])\n",
    "            age_name = age_pattern.match(str_name)\n",
    "\n",
    "            if birth_year_bio:\n",
    "                year = birth_year_bio.group(1)\n",
    "                age = 2019 - int(year)\n",
    "                user['age'] = str(age)\n",
    "                filtered_profiles.append(user)\n",
    "            elif birth_year_name:\n",
    "                year = birth_year_name.group(1)\n",
    "                age = 2019 - int(year)\n",
    "                user['age'] = str(age)\n",
    "                filtered_profiles.append(user)\n",
    "            elif age_bio:\n",
    "                user['age'] = age_bio.group(1)\n",
    "                filtered_profiles.append(user)\n",
    "            elif age_name:\n",
    "                user['age'] = age_name.group(1)\n",
    "                filtered_profiles.append(user)\n",
    "                \n",
    "with open(output_path, 'w') as outfile:\n",
    "    json.dump(filtered_profiles, outfile)\n",
    "    \n",
    "age_profiles = pd.DataFrame.from_dict(filtered_profiles)\n",
    "age_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scraping the profile pictures\n",
    "\n",
    "For the collected users having a bio valid with respect to the regex described above, we then downloaded the profile pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing users of canada\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/test/bio/filtered/canada.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f816f443e4c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# profiles JSON parsing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofiles_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0muser_profiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0musr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_profiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/test/bio/filtered/canada.json'"
     ]
    }
   ],
   "source": [
    "# image retrieval\n",
    "def url_to_image(url):\n",
    "    resp = urllib.urlopen(url)\n",
    "    return resp\n",
    "\n",
    "profiles_path = './data/test/bio/filtered/%s.json' %country\n",
    "img_path = './data/test/images/%s/' %country\n",
    "parsed_users = []\n",
    "\n",
    "print('Parsing users of', country)\n",
    "\n",
    "# profiles JSON parsing\n",
    "with open(profiles_path, 'r') as file:\n",
    "    user_profiles = json.load(file)\n",
    "    for usr in user_profiles[0:]:\n",
    "        url = usr['image']\n",
    "        username = usr['username']\n",
    "        image_path = img_path + username + '.jpg'\n",
    "    # download the image URL\n",
    "        if url != '':\n",
    "            print (\"Downloading image\")\n",
    "            image = url_to_image(url)\n",
    "            f = open(image_path,'wb')\n",
    "            f.write(image.read())\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Exploration\n",
    "\n",
    "// **add the age distribution graph for 38000**\n",
    "\n",
    "// **add the age distribution graph for 1900+ instagram data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution for Instagram\n",
    "\n",
    "image_nums = {}\n",
    "image_nums_all = {}\n",
    "\n",
    "for i in range(1,101):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    if i < 10:\n",
    "        i = '0' + str(i)\n",
    "        \n",
    "    # Fix the path and variables\n",
    "    data_dir = './data/processed/merged_raw/%s/' %i\n",
    "    if os.path.exists(data_dir):\n",
    "        os.listdir(data_dir)\n",
    "        image_num = len([name for name in os.listdir(data_dir)])\n",
    "        i = int(i)\n",
    "        image_nums[i] = image_num\n",
    "        image_nums_all[i] = image_num\n",
    "    else: \n",
    "        i = int(i)\n",
    "        image_nums_all[i] = 0\n",
    "        \n",
    "print(image_nums)\n",
    "vals = image_nums\n",
    "lists = sorted(image_nums_all.items()) \n",
    "x, y = zip(*lists)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Validation\n",
    "\n",
    "To perform the validation we used the opencv trained model for face detection. We ran the detection algorithm on all the collected images to eliminate the pictures which contain more or less than one face as shown below.\n",
    "Apart from face detection, the network also cuts the faces from the picture.\n",
    "We (after some more manual verification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/test/images/canada/@camybr_.jpg\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.1) /io/opencv/modules/dnn/src/caffe/caffe_io.cpp:1132: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"opencv_face_detector_uint8.pb\" in function 'ReadProtoFromBinaryFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-7be477f6ed7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetFaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfaces\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no face detected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ce7454-group3-project/src/preprocess/face_detection/face_utils.py\u001b[0m in \u001b[0;36mgetFaces\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mfaceModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"opencv_face_detector_uint8.pb\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mfaceNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaceModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaceProto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Open a video file or an image file or a camera stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.1) /io/opencv/modules/dnn/src/caffe/caffe_io.cpp:1132: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"opencv_face_detector_uint8.pb\" in function 'ReadProtoFromBinaryFile'\n"
     ]
    }
   ],
   "source": [
    "from src.preprocess.face_detection.face_utils import getFaces\n",
    "from os import walk\n",
    "\n",
    "image_path = './data/test/images/%s' %country\n",
    "\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(image_path):\n",
    "    for image in filenames:        \n",
    "        username = image.split('.jpg')[0].split('/')\n",
    "        name = username[len(username) - 1]        \n",
    "        save_path = './data/test/images/%s/cut/%s.jpg' %(country,name)\n",
    "        path = image_path + '/' + image\n",
    "        print(path)\n",
    "        faces = getFaces(path)\n",
    "        if faces == 0:\n",
    "            print('no face detected')\n",
    "        elif len(faces) == 2:\n",
    "            print('that is a couple')\n",
    "        elif len(faces) == 1:\n",
    "            print('there is one face') \n",
    "            cv.imwrite(save_path, faces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Preprocessing\n",
    "\n",
    "// **this section should describe the bin-size splitting thing, section 3 will use that**\n",
    "\n",
    "// **it should also contain Instagram data labelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/test/result/21_2_@maude_montpetit.jpg\n",
      "./data/test/result/20_2_@mariiepierp21.jpg\n",
      "./data/test/result/23_2_@camybr_.jpg\n",
      "./data/test/result/22_2_@1997kakou.jpg\n",
      "./data/test/result/16_2_@annesosimo.jpg\n",
      "./data/test/result/18_2_@alyson_cote.jpg\n",
      "./data/test/result/19_2_@coralie.sav.jpg\n",
      "./data/test/result/27_2_@audreyann.paquet.jpg\n"
     ]
    }
   ],
   "source": [
    "# Countries indexes mapping\n",
    "\n",
    "countries = {\n",
    "    \"australia\": \"0\",\n",
    "    \"brazil\": \"1\",\n",
    "    \"canada\": \"2\",\n",
    "    \"china\": \"3\",\n",
    "    \"ethiopia\": \"4\",\n",
    "    \"nigeria\": \"4\",\n",
    "    \"germany\": \"5\",\n",
    "    \"india\": \"6\",\n",
    "    \"indonesia\": \"7\",\n",
    "    \"iran\": \"8\",\n",
    "    \"russia\": \"9\"\n",
    "}\n",
    "\n",
    "image_path = './data/test/images/%s/cut/' %country\n",
    "data_path = './data/test/bio/filtered/%s.json' %country\n",
    "result_path = './data/test/result'\n",
    "\n",
    "with open(data_path, 'r') as file:\n",
    "    user_profiles = json.load(file)\n",
    "    for user in user_profiles:\n",
    "        username = user['username']\n",
    "        age = user['age']\n",
    "        country_index = countries[country]\n",
    "        \n",
    "        image = image_path + username + '.jpg'\n",
    "        \n",
    "        if os.path.exists(image):\n",
    "        # The new name is 'age_country_username'\n",
    "            new_name = '%s/%s_%s_%s.jpg' %(result_path, age, country_index, username)\n",
    "            print(new_name)\n",
    "            os.rename(image, new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models and Training\n",
    "\n",
    "In this section, we discuss the choosen models, the training configurations for each model, and the whole training pipeline. The outputs of this section are the saved trained weights for all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Selection\n",
    "\n",
    "We targeted on 3 representative models in face recognition and age prediction, the MLP, VGG, and ResNet.\n",
    "\n",
    "As there are many variants of these networks, the first thing is to determine which variants of these model are suitable for our project. \n",
    "We probed ResNet18, ResNet50, ResNet152 using parts of the training data (around 10,000) and found that the performance has no big difference. \n",
    "Thus we made the following selection:\n",
    "- ResNet18, resnet with 18 layers\n",
    "- VGG19_bn, vgg 19 layers with batch normalization\n",
    "- MLP18, 18-layer mlp\n",
    "\n",
    "The ResNet and VGG models can directly imported using the following statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torchvision.models import vgg19_bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the MLP model is implemented by ourself and you can find it in the `./src/neural_network/mlp.py` in the [project github](https://github.com/occia/ce7454-group3-project).\n",
    "\n",
    "For demo usage, here is a smaller version MLP implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class is for demo use\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, hidden_size4, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size2, hidden_size3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size3, hidden_size4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size4, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convert tensor (128, 1, 28, 28) --> (128, 1*28*28)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Training Parameters Setup\n",
    "\n",
    "We keep the following training configuration for all 3 choosen models:\n",
    "- Learning Rate, the initial value of learning rate is set as `0.001`\n",
    "- Optimizer, using **Adam** rather than **SGD**\n",
    "- Criterion, using `torch.nn.CrossEntropyLoss()`\n",
    "- Epoches, set to 50 as it balances the training time costs and the training consequence\n",
    "- Batch size, set as 256\n",
    "- Image pixels, set as `(3, 200, 200)`, 3 means 3 channels (a.k.a colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# training parameters setup for demo use\n",
    "#\n",
    "\n",
    "device= torch.device(\"cuda\")\n",
    "#device= torch.device(\"cpu\")\n",
    "\n",
    "channels = 3\n",
    "img_pixels = (200,200)\n",
    "lr = 0.001\n",
    "num_epochs = 2\n",
    "batch_size = 128\n",
    "\n",
    "# loading dataset\n",
    "def loading_dataset(train_dataset, test_dataset):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(img_pixels),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "    img_data_train = torchvision.datasets.ImageFolder(root=train_dataset, transform=transform)\n",
    "    data_loader_train = torch.utils.data.DataLoader(img_data_train, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    img_data_val = torchvision.datasets.ImageFolder(root=test_dataset, transform=transform)\n",
    "    data_loader_val = torch.utils.data.DataLoader(img_data_val, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = data_loader_train\n",
    "    dataloaders['val'] = data_loader_val\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Model Training WorkFlow\n",
    "\n",
    "The workflow is based on the template teacher provided in the class, and is improved in some aspects.\n",
    "\n",
    "Here lists the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# main training workflow\n",
    "#\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "    last = since\n",
    "    time_elapsed = since\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            time_elapsed = time.time() - last\n",
    "            last = time.time()\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} Time: {:.0f}m {:.0f}s'.format(phase, epoch_loss, epoch_acc, time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "            # deep copy the modeltopk\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of the networks are initialized randomly.\n",
    "Also the images of the dataset are shuffled every time.\n",
    "The key different parts of our implementation from the teacher's template are:\n",
    "- we do train & validation for every epoch\n",
    "- based on the validation result, we save the best epoch's weights, and return that instead of the one be trained longest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training Pipeline\n",
    "\n",
    "Till now, we know which model to train and how to train a model. To answer the questions we raised at the beginning, we need to train all the combinations of the selected models and the prepared datasets.\n",
    "\n",
    "Thus, the next step is building the training pipeline for all training combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download demo dataset\n",
    "#!wget -nc \"https://somelink\"\n",
    "#!ls\n",
    "#!tar xf ce7454_demo_dataset.tar.gz\n",
    "#!ls dataset\n",
    "#!mkdir -p ./saved_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] This training pipeline is for demo usage\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/demo_train_bin_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ac0123657cf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbinsize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbinsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloading_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./dataset/demo_train_bin_%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbinsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./dataset/demo_test_bin_%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbinsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"MLP\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ResNet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"VGG\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b5e511997fda>\u001b[0m in \u001b[0;36mloading_dataset\u001b[0;34m(train_dataset, test_dataset)\u001b[0m\n\u001b[1;32m     18\u001b[0m         transforms.ToTensor()])\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mimg_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdata_loader_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     91\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     92\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/demo_train_bin_1'"
     ]
    }
   ],
   "source": [
    "def training_and_save_model(net, num_epochs, model_save_name):\n",
    "    net = net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(net.parameters(), lr)\n",
    "    net, _ = train_model(net, dataloaders, criterion, optimizer, num_epochs)\n",
    "    torch.save(net.state_dict(), os.path.join(\"./saved_models/\", model_save_name))\n",
    "\n",
    "#\n",
    "# whole training pipeline\n",
    "#\n",
    "print(\"[+] This training pipeline is for demo usage\")\n",
    "for binsize in [1, 6, 10]:\n",
    "    classes = int((100 + binsize - 1) / binsize)\n",
    "    \n",
    "    dataloaders = loading_dataset(\"./dataset/demo_train_bin_%d\" % (binsize), \"./dataset/demo_test_bin_%d\" % (binsize))\n",
    "    \n",
    "    for model in [\"MLP\", \"ResNet\", \"VGG\"]:\n",
    "        print(\"[+] Training for %s with binsize %d dataset started\" % (model, binsize))\n",
    "        \n",
    "        if model == \"MLP\":\n",
    "            net = MLP(channels * img_pixels[0] * img_pixels[1], 512, 512, 512, 512, classes)\n",
    "        elif model == \"ResNet\":\n",
    "            net = resnet18(num_classes=classes)\n",
    "            # comment this as this is a demo\n",
    "            #continue\n",
    "        else:\n",
    "            net = vgg19_bn(num_classes=classes)\n",
    "            # comment this as this is a demo\n",
    "            #continue\n",
    "        \n",
    "        model_save_name = \"%s_%s_demo_merged_train_bin%d\" % (num_epochs, net.__class__.__name__, binsize)\n",
    "        training_and_save_model(net, num_epochs, model_save_name)\n",
    "\n",
    "        print(\"[+] Training for %s with binsize %d dataset done\" % (model, binsize))\n",
    "\n",
    "        del net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the pipeline code, we saved weights of the best epoch for all the models towards all the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "### 4.1 Accuracy Comparison Among Models\n",
    "\n",
    "### 4.2 Accuracy in predicting age (age total/18 years)\n",
    "\n",
    "### 4.3 Age perceived (age total/18 years x general/countries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
